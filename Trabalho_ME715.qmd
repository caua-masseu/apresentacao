---
title: 'Dados em Painéis '
subtitle: ''
author: 'Cauã Pereira Masseu 204238 <br> Luca Rocha 204326 <br> Paula Liserre 242782 <br> Victor Poggetti 204384'
institute: '<br> Instituto de Matemática, Estatística e Computação Científica (IMECC) <br> Universidade Estadual de Campinas (UNICAMP)'
format:
  revealjs:
    # html-math-method: katex
    embed-resources: true
    width: 1600
    height: 900
    code-fold: true
    theme: [default, C:\Users\PC\Desktop\styles.scss]
    title-slide-attributes:
     # data-background-image: 'figuras/unicamp.png'
      data-background-size: 18%
      data-background-position: 96% 5%
      data-background-opacity: '1'
lang: pt
crossref:
  eq-prefix: ''
knitr:
    opts_chunk: 
      fig.align: 'center'
editor_options: 
  chunk_output_type: console
---

# Dados em Painéis

## Motivação

- O que são dados em painéis? O que fazer quando temos dados em painéis? Como ajustar um modelo nesse tipo de dados?
- Dados em painel são um tipo de dados que combinam características de dados transversais e de séries temporais. Eles consistem em observações coletadas de múltiplas unidades ao longo de vários períodos de tempo, ou seja, n unidades de corte transversal que são acompanhadas ao longo de T períodos de tempo.

### Exemplo de Dados em Painel 

| ID | Y  | X_1 | X_2 |
|----|----|-----|-----|
| 1  | 10 | 5   | 7   |
| 2  | 15 | 6   | 9   |
| 1  | 12 | 7   | 8   |
| 2  | 14 | 5   | 10  |

## Tipos de dados de Painéis

- **Painel Balanceado**: Igual o número de observações para cada individuo observado (o qual terá maior ênfase)
- **Micro Painel**: Número de individuos é maior que o numero de periodos
- **Macro Painel**: Número de periodos é maior que o numero de individuos
- **Painel Dinâmico**: Inclui variáveis lagged de outras variáveis
- **Painel Não Balanceado**: número de observações diferentes para cada individuo observado

## Vantagens e Desvantagens dos Dados em Painel

TALVEZ ADICIONAR MAIS VANTAGENS E DESVANTAGENS

**Vantagens:**

- Controle de heterogeneidade não observada;
- Redução de colinearidade entre variáveis independentes;
- Melhor eficiência das estimativas;
- Observar uma mesma população ao longo do tempo

**Desvantagens:**

- Problemas de consistência e acessibilidade dos dados;
- Sazonalidade e choques externos;
- Modelos complexos e alta demanda computacional
- A presença de dados faltantes em um painel pode complicar a análise

## Modelo Geral para Dados em Painel

O modelo geral para dados em painel pode ser representado através da seguinte equação:

$y_{it} = \beta_{0it} + \beta_{1it}x_{1it} + \ldots + \beta_{kit}x_{kit} + \epsilon_{it}$

em que:

- **i** representa os diferentes indivíduos;
- **t** representa o período de tempo que está sendo analisado;
- **$y_{it}$** é o valor da variável dependente para a unidade i no tempo t (i = 1, ..., n e t = 1, ..., T);
- **$x_{kit}$** é o valor da k-ésima variável explicativa para o indivíduo i no instante de tempo t;
- **$e_{it}$** é a perturbação aleatória para a unidade i no tempo t (erros aleatórios);
- Caso $\epsilon_{it} \sim IID(0, \sigma^2)$, podemos estimar o modelo por MQO

## Forma Matricial do modelo

O modelo geral pode também ser representado pela forma matricial:

$y = X\beta + \epsilon$

em que:
- $y = [y_1 \ldots y_n]'$, com dimensão nT x 1;  
- $X = [X_1 \ldots X_n]'$, com dimensão nT x K;  
- $\beta = [\beta_1 \ldots \beta_K]'$, com dimensão K x 1;  
- $\epsilon = [\epsilon_1 \ldots \epsilon_n]$, com dimensão nT x 1;  
- $\epsilon = \alpha_i + \eta_{it}$;  
- $\alpha_i \sim N(0, \sigma^2_\alpha)$;  
- $\eta_{it} \sim N(0, \sigma^2_\eta)$;  
- Assume-se que Cor($\eta_{it}, X_{it}$) = 0;  
  - Se Cor($\alpha_i, X_{it}$) = 0 $\rightarrow$ \textbf{Modelo de Efeitos Aleatórios};   
  - Se Cor($\alpha_i, X_{it}$) $\neq$ 0 $\rightarrow$ \textbf{Modelo de Efeitos Fixos}

## Modelo de Efeitos Aleatórios

Este modelo considera que o intercepto seja uma variável aleatória e não uma constante, variando de um indivíduo para outro, mas não ao longo do tempo, e os parâmetros de resposta são constantes para todos os indivíduos e em todos os períodos de tempo.

Seja o modelo

$y_{it} = X_{it}\beta + \epsilon_{it}$

com $\epsilon_{it} = \alpha_i + \eta_{it}$:  
- O fato de $\alpha_i$ e $\eta_{it}$ não serem correlacionados com $X_{it}$ implica que não há correlação entre $\epsilon_{it}$ e $X_{it}$;  
- $\hat{\beta}_{MQO}$ é não viesado   
- A variância de $\hat{\beta}_{MQO}$ não está corretamente estimada  
- O estimador não é eficiente  
- MQO trata os efeitos $\alpha_i$ como parte do erro  
- Deve-se utilizar MQGF para estimar $\beta$  

## Suposições acerca do erro

Para o Modelo de Efeitos Aleatórios, tem-se algumas suposições envolvendo o erro aleatório:

- E[$\eta \mid X$] = 0
- E[$\eta\eta' \mid X$] = $\sigma^2_{\eta} I_{nT}$
- E[$\alpha_i\alpha_j \mid X$] = 0, i $\neq$ j
- E[$\alpha_i\alpha_i \mid X$] = $\sigma^2_{\alpha}$
- E[$\alpha_i\mid X$] = 0
- E[$\alpha_1\eta_{jt} \mid X$] = 0

Com essas suposições, é possível obter a covariância do termo de erro:

E[$\epsilon_i\epsilon_i'$] = $\sigma^2_\alpha ii' + \sigma^2_\eta I_T$

Com isso,

$\Omega$ = E[$\epsilon\epsilon'$] = $I_n \bigotimes \Sigma$, em que $\Sigma$ = E[$\epsilon_i\epsilon_i'$]

## Between e Within Estimator

Sejam $\textbf{D}_{nTxn}$ uma matriz formada por n variáveis dummy representando cada unidade de i, e $\textbf{P}_{D} = \textbf{D}(\textbf{D}'\textbf{D})^{-1}\textbf{D}'$ uma matriz simétrica e idempotente.

- $\textbf{P}_{D}y = \textbf{P}_{D}X\beta + \textbf{P}_{D}\epsilon$
- $\hat{\beta_B} = (X'\textbf{P}_{D}'\textbf{P}_{D}X)^{-1}X'\textbf{P}_{D}'\textbf{P}_{D}y = (X'\textbf{P}_{D}X)^{-1}X'\textbf{P}_{D}y$

em que $\hat{\beta_B}$ é chamado de between estimator, pois o estimador utiliza a informação **entre** indivíduos.

Utilizando a matriz simétrica e idempotente $M_D = I_{nT} - P_D$ no lugar de $\textbf{P}_D$:

- $\textbf{M}_{D}y = \textbf{M}_{D}X\beta + \textbf{M}_{D}\epsilon$
- $\hat{\beta_W} = (X'M_DX)^{-1}X'M_Dy$

em que $\hat{\beta_W}$ é chamado de within estimator, pois o estimador utiliza a informação **dentro** de cada indivíduo i.

## Estimação de $\sigma^2_\eta$ e $\sigma^2_\alpha$

O modelo de erros é dado por:

$\epsilon_{it} = \alpha_i + \eta_{it},$

A variância total do erro $\epsilon_{it}$ é:

$Var(\epsilon_{it}) = \sigma^2_\eta +\sigma^2_\alpha$

Ajusta-se um modelo de regressão e calcula-se os resíduos $\hat{u_{it}}$ correspondentes.

- Resíduos "Dentro" ($\hat{u_W}$): Diferença entre os resíduos individuais e a média ao longo do tempo.
- Resíduo "Entre" ($\hat{u_B}$): Média dos resíduos por unidade.

Calculando os estimadores $\hat{\sigma^2_\eta}$ e $\hat{\sigma^2_\alpha}$, temos:

- $\hat{\sigma^2_\eta} = \frac{\hat{u'_W} \hat{u_W}}{nT - nk - n}$;
- $\hat{\sigma^2_\alpha} = \frac{\hat{u'_B}\hat{u_B}}{n - k} - \frac{\hat{\sigma^2_\eta}}{T}$.

## Modelo de Efeitos Fixos

O modelo de efeitos fixos pretende controlar os efeitos das variáveis omitidas que variam entre indivíduos e permanecem constantes ao longo do tempo. Para isto, supõe que o intercepto varia de um indivíduo para o outro, mas é constante ao longo do tempo; ao passo que os parâmetros resposta são constantes para todos os indivíduos e em todos os períodos de tempo.

O modelo de efeitos fixos pode ser dado por:

$y_{it} = \alpha_i + \beta_1x_{1it} + \ldots +\beta_kx_{kit} + \epsilon_{it}$

Pela forma matricial, empilhando todas as observações para todas as unidades e períodos, o modelo pode ser escrito como:

$y = D\alpha + X\beta + \epsilon$

As suposições acerca do erro são as mesmas apresentadas no Modelo de Efeitos Aleatórios.

## Modelo de Efeitos Fixos

- Com dados em painel é possível obter estimadores não viesados de parâmetros de interesse mesmo diante do efeito omitido correlacionado. Coisa que MQO no corte transversal de individuos não consegue.
- Com um estimador de efeitos fixos, não podemos recuperar estimadores de variáveis explicataivas invariantes no tempo.
- O estimador de efeitos fixos é robusto à omissão de regressores (relevantes) que sejam invariantes no tempo;
- Quando o efeito aleatório é válido, o estimador de efeitos fixos ainda produz estimadores consistentes (mas não eficiente) dos parâmetros de interesse.

## Modelo de Efeitos Aleatórios vs Modelo de Efeitos Fixos

- Se a correlação entre $\alpha_i$ e $X_{it}$ for nula, deve-se utilizar efeitos aleatórios. Caso contrário, utiliza-se efeitos fixos;
- Teste de Durbin-Wu-Hausman pode ajudar nessa escolha

O teste Durbin-Wu-Hausman avalia a consistência de um estimador quando comparado a um estimador alternativo, menos eficiente, que já é conhecido por ser consistente, ajudando a avaliar se um modelo estatístico corresponde aos dados.

- $H_0$: Efeitos Aleatórios (RE) são preferidos;
- $H_1$: Efeitos Fixos (FE) são preferidos.
- Estatística do Teste: $H = (\hat{\beta_{RE}} - \hat{\beta_{FE}})'(\Sigma_{FE} - \Sigma_{RE})^{-1}(\hat{\beta_{RE}} - \hat{\beta_{FE}}) \sim \chi^2_k$

| Estimador          | $H_0$ é verdadeira      | $H_0$ não é verdadeira |
|--------------------|---------------------------|--------------------------|
| $\hat{\beta_{RE}}$ | Consistente Eficiente   | Inconsistente            |
| $\hat{\beta_{FE}}$ | Consistente Ineficiente | Consistente              |


# Aplicação

Cornwell, C., & Trumbull, W. N. (1994). Estimating the Economic Model of Crime with Panel Data. The Review of Economics and Statistics, 76(2), 360–366.

## Motivação

- **Histórico e Contexto**: Desde o artigo seminal de Bucker em 1968, a economia do crime tem sido um campo de estudo significativo. No entanto, a maioria das pesquisas utiliza dados agregados, apesar de o modelo econômico do crime ser mais adequado para dados individuais.

- **Desafios Metodológicos**: A criação de amostras aleatórias representativas é cara e complexa, limitando a análise com dados individuais. Apesar das críticas, estudos com dados agregados têm influenciado políticas públicas.

- **Efeito Dissuasor da Punição**: A literatura empírica geralmente concorda que a certeza e a severidade da punição têm um forte efeito dissuasor sobre o crime. Este estudo, no entanto, busca evidências de que essa capacidade de dissuasão pode ser menor do que se supõe.

- **Vantagens dos Dados em Painel**: Utilizando dados em painel de condados, podemos realizar uma análise mais detalhada, controlando características não observáveis que podem influenciar as variáveis do sistema de justiça. Isso ajuda a evitar as estimativas inconsistentes comuns em estudos transversais.

- **Endogeneidade e Estimativas Consistentes**: Dados em painel permitem controlar características específicas de cada condado que não são observáveis, mas que podem estar correlacionadas com as variáveis do modelo. Ignorar esses fatores pode levar a estimativas inconsistentes. Estudos anteriores, baseados em dados transversais, frequentemente negligenciam essa "endogeneidade", mesmo quando utilizam modelos de equações simultâneas.

## Modelo Econômico do Crime

- **Princípio Básico**: Indivíduos maximizam a utilidade esperada ao decidir participar de atividades criminosas, considerando os benefícios e custos das atividades ilegais.

- **Equação de Crime**:
  $R_{it} = X_{it}\beta + P_{it}\gamma + \alpha_i + \epsilon_{it}$
  - $R_{it}$: Taxa de criminalidade
  - $X_{it}$: Variáveis que controlam o retorno relativo às oportunidades legais
  - $P_{it}$: Variáveis dissuasoras (proxies para PA, Pc, PP e S)
  - $\alpha_i$: Efeitos fixos dos condados
  - $\epsilon_{it}$: Termos de perturbação (iid, média zero, variância constante $\sigma^2$)

## Estimação e Transformações

- **Transformações "Between" e "Within"**:
  - **Seção Transversal**: 
    $R_i = X_i\beta + P_i\gamma + \alpha_i + \epsilon_i$
  - **Painel**: 
    $R_{it} = X_{it}\beta + P_{it}\gamma + \nu_{it}$

- **Desafios com OLS e 2SLS**:
  - Negligenciam a heterogeneidade não observada.
  - Estimativas inconsistentes se $\alpha_i$ estiver correlacionado com $(X_{it}, P_{it})$.

- **Abordagem de Estimação**:
  - **Estimador "Within"**: Lida com a correlação entre variáveis e heterogeneidade não observada.
  - **2SLS com Transformação "Between"**: Trata simultaneidade convencional.

## Banco de Dados

::: columns
:::: column

- **Fonte**: Comissão de Segurança de Emprego da Carolina do Norte, com 630 observações e 25 variáveis

**Principais Variáveis:**
- **county**: ID do condado  
- **year**: Ano (81-87)  
- **crmrte**: Crimes per capita  
- **prbarr**: Probabilidade de prisão(tipo uma detenção, uma prisão em menos tempo) 
- **prbconv**: Probabilidade de condenação  
- **prbpris**: Probabilidade de prisão pós condenação 
- **avgsen**: Sentença média (dias)  
- **polpc**: Policiais per capita  
- **density**: Pessoas/milha²  
- **taxpc**: Receita tributária per capita  
- **urban**: 1 se em SMSA  
- **pctmin80**: % Minorias em 1980
::::

:::: column

**Salários Semanais:**  
- **wcon**: Construção  
- **wtuc**: Transporte, utilidades, comunicação  
- **wtrd**: Comércio  
- **wfir**: Finanças, seguros, imóveis  
- **wser**: Serviços  
- **wmfg**: Manufatura  
- **wfed**: Funcionários federais  
- **wsta**: Funcionários estaduais  
- **wloc**: Funcionários locais  

**Outras Variáveis:**  
- **mix**: Mistura de ofensas  
- **pctymle**: % Jovens do sexo masculino

::::

:::

```{r, echo = FALSE}
knitr::opts_chunk$set(python.reticulate = F)
```

## Modelo Betwenn

Em cada caso, a transformação log foi utilizada nas variáveis, assim os coeficientes são interpretados como elásticos

::: {.panel-tabset}

### R

```{r, echo = T, warning = F, message = F}
library(plm); library(wooldridge)
library(dplyr); library(AER)

data <- crime4
data['pctmin80'] = data['pctmin80']/100

crime_panel = pdata.frame(data, index = c("county", "year"))

model_between = plm(formula(lcrmrte ~ lprbarr + lprbconv + lprbpris + lavgsen + lpolpc + ldensity + 
                              lpctymle + lwcon + lwtuc + lwtrd + lwfir + lwser + lwmfg + lwfed + 
                              lwsta + lwloc + west + central + urban + lpctmin), data = crime_panel, model = "between")

summary(model_between)
```

### Python

```{python, echo = T, warning = F, message = F}
import wooldridge as woo
from linearmodels.panel import PanelOLS
from linearmodels.panel import BetweenOLS

crime = woo.data('crime4')
crime.set_index(['county', 'year'], inplace=True)

model_between = BetweenOLS.from_formula('lcrmrte ~ 1 + lprbarr + lprbconv + lprbpris + lavgsen + lpolpc + ldensity + lpctymle + lwcon + lwtuc + lwtrd + lwfir + lwser + lwmfg + lwfed + lwsta + lwloc + west + central + urban + lpctmin + EntityEffects', crime)

results_between = model_between.fit()
print(results_between)
```


### Julia 

```{julia, echo = T, warning = F, message = F}
using DataFrames, GLM, WooldridgeDatasets, Statistics

crime = DataFrame(wooldridge("crime4"));
crime.pctmin80 .= crime.pctmin80 ./ 100;

# Calculate group means for each county
group_means = combine(groupby(crime, :county), names(crime) .=> mean, renamecols=false);

# Define the formula for the between model
formula = @formula(lcrmrte ~ lprbarr + lprbconv + lprbpris + lavgsen + lpolpc + ldensity + lpctymle + lwcon + lwtuc + lwtrd + lwfir + lwser + lwmfg + lwfed + lwsta + lwloc + west + central + urban + lpctmin);

# Fit the between-effects model using the group means
model_between = lm(formula, group_means);
coeftable(model_between)
```

:::

## Modelo Within

::: {.panel-tabset}

### R

```{r, echo = T, warning = F, message = F}
model_within = plm(formula(lcrmrte ~ lprbarr + lprbconv + lprbpris + lavgsen + lpolpc + ldensity + 
                             lpctymle + lwcon + lwtuc + lwtrd + lwfir + lwser + lwmfg + lwfed + 
                             lwsta + lwloc + west + central + urban + lpctmin) ,  data = crime_panel, model = "within")

summary(model_within)
phtest(model_between, model_within)
```

### Python

```{python, echo = T, warning = F, message = F}
import statsmodels.api as sm
import wooldridge as woo
from linearmodels.panel import PanelOLS
from linearmodels.panel import BetweenOLS
from scipy import stats

crime = woo.data('crime4')
crime.set_index(['county', 'year'], inplace=True)

model_within = PanelOLS.from_formula(
    'lcrmrte ~ lprbarr + lprbconv + lprbpris + lavgsen + lpolpc + ldensity + lpctymle + lwcon + lwtuc + lwtrd + lwfir + lwser + lwmfg + lwfed + lwsta + lwloc + west + central + urban + lpctmin + EntityEffects',
    crime, drop_absorbed=True
)

results_within = model_within.fit()
print(results_within)
```

### Julia

```{julia, echo=T, warning=F, message=F}
dependent_var = "lcrmrte";
independent_vars = ["lprbarr", "lprbconv", "lprbpris", "lavgsen", "lpolpc", "ldensity",
                    "lpctymle", "lwcon", "lwtuc", "lwtrd", "lwfir", "lwser", "lwmfg",
                    "lwfed", "lwsta", "lwloc", "west", "central", "urban", "lpctmin"];

# Group by 'county'
crime_panel = crime ;
grouped_data = groupby(crime_panel, :county);

# Function to demean a column within groups
function demean_within_group(data, column)
    group_means = combine(grouped_data, column => mean => :mean);
    demeaned = similar(data[!, column]);
    for (i, row) in enumerate(eachrow(data))
        group_mean = group_means[group_means.county .== row.county, :mean][1];
        demeaned[i] = row[column] - group_mean;
    end
    return demeaned;
end

# Demean the dependent variable
y_demeaned = demean_within_group(crime_panel, dependent_var);

# Demean the independent variables
X_demeaned = DataFrame();
for var in independent_vars
    X_demeaned[!, var] = demean_within_group(crime_panel, var);
end

# Add the demeaned dependent variable to the DataFrame
X_demeaned[!, dependent_var] = y_demeaned;

# Construct the formula for the regression
formula = @formula(lcrmrte ~ lprbarr + lprbconv + lprbpris + lavgsen + lpolpc + ldensity +
                   lpctymle + lwcon + lwtuc + lwtrd + lwfir + lwser + lwmfg +
                   lwfed + lwsta + lwloc + west + central + urban + lpctmin);

model_within = lm(formula, X_demeaned);
coeftable(model_within)
```

:::

## Modelo Within - IV

Utilizamos as variáveis instrumentais mix de infrações e receita tributária per capita (taxpc) para substituir lprbarr e lpolpc:  
- Mix de Infrações: Proporção de crimes com contato direto, usada para capturar a variação na probabilidade de prisão sem afetar diretamente a taxa de criminalidade.
- Receita Tributária per Capita: Reflete a disposição dos condados em financiar forças policiais maiores, independente da taxa de criminalidade, servindo como um instrumento para lpolpc

::: {.panel-tabset}

### R

```{r, echo = T, warning = F, message = F}
variables <- c("lcrmrte", "lprbarr", "lpolpc", "lprbpris", "lavgsen",
               "ldensity", "lprbconv", "lpctymle", "lwcon", "lwtuc",
               "lwtrd", "lwfir", "lwser", "lwmfg", "lwfed", "lwsta",
               "lwloc", "west", "central", "urban", "lpctmin",
               "lmix", "ltaxpc")

crime_panel_demeaned <- crime_panel %>% group_by(county) %>%
  mutate(across(all_of(variables), ~ . - mean(.), .names = "{col}_demeaned"))

instruments_demeaned <- c("lprbconv_demeaned", "lprbpris_demeaned", "lavgsen_demeaned",
                          "ldensity_demeaned", "lpctymle_demeaned", "lwcon_demeaned",
                          "lwtuc_demeaned", "lwtrd_demeaned", "lwfir_demeaned",
                          "lwser_demeaned", "lwmfg_demeaned", "lwfed_demeaned",
                          "lwsta_demeaned", "lwloc_demeaned", "west_demeaned",
                          "central_demeaned", "urban_demeaned", "lpctmin_demeaned",
                          "lmix_demeaned", "ltaxpc_demeaned")

first_stage_lprbarr <- lm(lprbarr_demeaned ~ ., data = crime_panel_demeaned[, c("lprbarr_demeaned", instruments_demeaned)])
crime_panel_demeaned$lprbarr_hat <- predict(first_stage_lprbarr)

first_stage_lpolpc <- lm(lpolpc_demeaned ~ ., data = crime_panel_demeaned[, c("lpolpc_demeaned", instruments_demeaned)])
crime_panel_demeaned$lpolpc_hat <- predict(first_stage_lpolpc)


exogenous_vars_demeaned <- c("lprbpris_demeaned", "lavgsen_demeaned",
                             "ldensity_demeaned", "lprbconv_demeaned",
                             "lpctymle_demeaned", "lwcon_demeaned",
                             "lwtuc_demeaned", "lwtrd_demeaned",
                             "lwfir_demeaned", "lwser_demeaned",
                             "lwmfg_demeaned", "lwfed_demeaned",
                             "lwsta_demeaned", "lwloc_demeaned",
                             "west_demeaned", "central_demeaned",
                             "urban_demeaned", "lpctmin_demeaned")

second_stage_formula <- as.formula(paste("lcrmrte_demeaned ~ -1 + lprbarr_hat + lpolpc_hat +",
                                         paste(exogenous_vars_demeaned, collapse = " + ")))

second_stage <- lm(second_stage_formula, data = crime_panel_demeaned)

summary(second_stage)
```


### Python 

```{python, echo=T, warning=F, message=F}
import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import ols
import wooldridge as woo
from scipy import stats

crime = woo.data('crime4')
crime.set_index(['county', 'year'], inplace=True)

# Assuming 'crime_panel' is a pandas DataFrame that contains your data, including a 'county' column

# List of variables
variables = ["lcrmrte", "lprbarr", "lpolpc", "lprbpris", "lavgsen",
           "ldensity", "lprbconv", "lpctymle", "lwcon", "lwtuc",
           "lwtrd", "lwfir", "lwser", "lwmfg", "lwfed", "lwsta",
           "lwloc", "west", "central", "urban", "lpctmin",
           "lmix", "ltaxpc"]

crime_panel = crime.copy()
crime_panel_demeaned = crime_panel.copy()

grouped = crime_panel.groupby('county')
for var in variables:
  group_mean = grouped[var].transform('mean')
  # Subtract the group mean from the variable to get the demeaned variable
  crime_panel_demeaned[var + '_demeaned'] = crime_panel[var] - group_mean

# Define the demeaned instruments
instruments_demeaned = [
  "lprbconv_demeaned", "lprbpris_demeaned", "lavgsen_demeaned",
  "ldensity_demeaned", "lpctymle_demeaned", "lwcon_demeaned",
  "lwtuc_demeaned", "lwtrd_demeaned", "lwfir_demeaned",
  "lwser_demeaned", "lwmfg_demeaned", "lwfed_demeaned",
  "lwsta_demeaned", "lwloc_demeaned", "west_demeaned",
  "central_demeaned", "urban_demeaned", "lpctmin_demeaned",
  "lmix_demeaned", "ltaxpc_demeaned"
]

# First-stage regression for lprbarr_demeaned
first_stage_formula_lprbarr = 'lprbarr_demeaned ~ ' + ' + '.join(instruments_demeaned)
first_stage_lprbarr = ols(first_stage_formula_lprbarr, data=crime_panel_demeaned).fit()

crime_panel_demeaned['lprbarr_hat'] = first_stage_lprbarr.predict(crime_panel_demeaned)

# First-stage regression for lpolpc_demeaned
first_stage_formula_lpolpc = 'lpolpc_demeaned ~ ' + ' + '.join(instruments_demeaned)
first_stage_lpolpc = ols(first_stage_formula_lpolpc, data=crime_panel_demeaned).fit()

# Predicted values of lpolpc_demeaned
crime_panel_demeaned['lpolpc_hat'] = first_stage_lpolpc.predict(crime_panel_demeaned)

# Define the exogenous variables for the second stage (demeaned)
exogenous_vars_demeaned = [
  "lprbpris_demeaned", "lavgsen_demeaned", "ldensity_demeaned",
  "lprbconv_demeaned", "lpctymle_demeaned", "lwcon_demeaned",
  "lwtuc_demeaned", "lwtrd_demeaned", "lwfir_demeaned",
  "lwser_demeaned", "lwmfg_demeaned", "lwfed_demeaned",
  "lwsta_demeaned", "lwloc_demeaned", "west_demeaned",
  "central_demeaned", "urban_demeaned", "lpctmin_demeaned"
]

# Second-stage regression formula
second_stage_formula = 'lcrmrte_demeaned ~ -1 + lprbarr_hat + lpolpc_hat + ' + ' + '.join(exogenous_vars_demeaned)

# Second-stage regression
second_stage = ols(second_stage_formula, data=crime_panel_demeaned).fit()

# Display the summary of the second-stage regression
print(second_stage.summary())
```


### Julia

```{julia, echo=T, warning=F, message=F}
using DataFrames, Statistics, WooldridgeDatasets

variables = ["lcrmrte", "lprbarr", "lpolpc", "lprbpris", "lavgsen",
             "ldensity", "lprbconv", "lpctymle", "lwcon", "lwtuc",
             "lwtrd", "lwfir", "lwser", "lwmfg", "lwfed", "lwsta",
             "lwloc", "west", "central", "urban", "lpctmin",
             "lmix", "ltaxpc"];

grouped_crime = groupby(crime, :county);

for v in variables
    demeaned_col = Symbol(v * "_demeaned");
    crime[!, demeaned_col] = vcat([g[!, v] .- mean(g[!, v]) for g in grouped_crime]...);
end


# First-stage regression for lprbarr_demeaned
first_stage_formula_lprbarr = @formula(lprbarr_demeaned ~ lprbconv_demeaned + lprbpris_demeaned + lavgsen_demeaned +
                                       ldensity_demeaned + lpctymle_demeaned + lwcon_demeaned +
                                       lwtuc_demeaned + lwtrd_demeaned + lwfir_demeaned +
                                       lwser_demeaned + lwmfg_demeaned + lwfed_demeaned +
                                       lwsta_demeaned + lwloc_demeaned + west_demeaned +
                                       central_demeaned + urban_demeaned + lpctmin_demeaned + lmix + ltaxpc);

first_stage_lprbarr = lm(first_stage_formula_lprbarr, crime);
crime[!, :lprbarr_hat] = predict(first_stage_lprbarr);

# First-stage regression for lpolpc_demeaned
first_stage_formula_lpolpc = @formula(lpolpc_demeaned ~ lprbconv_demeaned + lprbpris_demeaned + lavgsen_demeaned +
                                       ldensity_demeaned + lpctymle_demeaned + lwcon_demeaned +
                                       lwtuc_demeaned + lwtrd_demeaned + lwfir_demeaned +
                                       lwser_demeaned + lwmfg_demeaned + lwfed_demeaned +
                                       lwsta_demeaned + lwloc_demeaned + west_demeaned +
                                       central_demeaned + urban_demeaned + lpctmin_demeaned + lmix + ltaxpc);

first_stage_lpolpc = lm(first_stage_formula_lpolpc, crime);
crime[!, :lpolpc_hat] = predict(first_stage_lpolpc);

# Second-stage regression
second_stage_formula = @formula(lcrmrte_demeaned ~  lprbarr_hat + lpolpc_hat + 
                                 lprbpris_demeaned + lavgsen_demeaned + ldensity_demeaned +
                                 lprbconv_demeaned + lpctymle_demeaned + lwcon_demeaned +
                                 lwtuc_demeaned + lwtrd_demeaned + lwfir_demeaned +
                                 lwser_demeaned + lwmfg_demeaned + lwfed_demeaned +
                                 lwsta_demeaned + lwloc_demeaned + west_demeaned +
                                 central_demeaned + urban_demeaned + lpctmin_demeaned);

second_stage = lm(second_stage_formula, crime);
coeftable(second_stage)
```

:::

## Modelo Between - IV

::: {.panel-tabset}

### R

```{r, echo = T, warning = F, message = F}
between_data <- crime_panel %>%
  group_by(county) %>%
  summarise(
    lcrmrte = mean(lcrmrte, na.rm = TRUE),
    lprbarr = mean(lprbarr, na.rm = TRUE),
    lpolpc = mean(lpolpc, na.rm = TRUE),
    lprbpris = mean(lprbpris, na.rm = TRUE),
    lavgsen = mean(lavgsen, na.rm = TRUE),
    ldensity = mean(ldensity, na.rm = TRUE),
    lprbconv = mean(lprbconv, na.rm = TRUE),
    lpctymle = mean(lpctymle, na.rm = TRUE),
    lwcon = mean(lwcon, na.rm = TRUE),
    lwtuc = mean(lwtuc, na.rm = TRUE),
    lwtrd = mean(lwtrd, na.rm = TRUE),
    lwfir = mean(lwfir, na.rm = TRUE),
    lwser = mean(lwser, na.rm = TRUE),
    lwmfg = mean(lwmfg, na.rm = TRUE),
    lwfed = mean(lwfed, na.rm = TRUE),
    lwsta = mean(lwsta, na.rm = TRUE),
    lwloc = mean(lwloc, na.rm = TRUE),
    west = mean(west, na.rm = TRUE),
    central = mean(central, na.rm = TRUE),
    urban = mean(urban, na.rm = TRUE),
    lpctmin = mean(lpctmin, na.rm = TRUE),
    lmix = mean(lmix, na.rm = TRUE),
    ltaxpc = mean(ltaxpc, na.rm = TRUE)
  )

endogenous_vars <- c("lprbarr", "lpolpc")

exogenous_vars <- c("lprbpris", "lavgsen", "ldensity", "lprbconv", "lpctymle",
                    "lwcon", "lwtuc", "lwtrd", "lwfir", "lwser", "lwmfg", "lwfed",
                    "lwsta", "lwloc", "west", "central", "urban", "lpctmin")

additional_instruments <- c("lmix", "ltaxpc")
instruments <- c(exogenous_vars, additional_instruments)

rhs_formula <- paste(c(endogenous_vars, exogenous_vars), collapse = " + ")
iv_formula <- paste(instruments, collapse = " + ")

formula_iv <- as.formula(paste("lcrmrte ~", rhs_formula, "|", iv_formula))

model_2sls_between <- ivreg(formula_iv, data = between_data)

summary(model_2sls_between)
```

### Python

```{python, echo=T, warning=F, message=F}
import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import ols
import wooldridge as woo
from scipy import stats

data = woo.data('crime4')
data.set_index(['county', 'year'], inplace=True)
crime_panel = data.copy()
crime_panel_demeaned = crime_panel.copy()

variables = ["lcrmrte", "lprbarr", "lpolpc", "lprbpris", "lavgsen",
           "ldensity", "lprbconv", "lpctymle", "lwcon", "lwtuc",
           "lwtrd", "lwfir", "lwser", "lwmfg", "lwfed", "lwsta",
           "lwloc", "west", "central", "urban", "lpctmin",
           "lmix", "ltaxpc"]

between_data = crime_panel.groupby('county')[variables].mean().reset_index()

endogenous_vars = ['lprbarr', 'lpolpc']

exogenous_vars = [
  'lprbpris', 'lavgsen', 'ldensity', 'lprbconv', 'lpctymle',
  'lwcon', 'lwtuc', 'lwtrd', 'lwfir', 'lwser', 'lwmfg',
  'lwfed', 'lwsta', 'lwloc', 'west', 'central', 'urban', 'lpctmin'
]

additional_instruments = ['lmix', 'ltaxpc']

instruments = exogenous_vars + additional_instruments

# Prepare the data for the first-stage regression of lprbarr
X_first_stage = between_data[instruments]
X_first_stage = sm.add_constant(X_first_stage)
Y_first_stage = between_data['lprbarr']

first_stage_lprbarr = sm.OLS(Y_first_stage, X_first_stage).fit()
between_data['lprbarr_hat'] = first_stage_lprbarr.predict(X_first_stage)

Y_first_stage = between_data['lpolpc']
first_stage_lpolpc = sm.OLS(Y_first_stage, X_first_stage).fit()
between_data['lpolpc_hat'] = first_stage_lpolpc.predict(X_first_stage)


X_second_stage = between_data[['lprbarr_hat', 'lpolpc_hat'] + exogenous_vars]
X_second_stage = sm.add_constant(X_second_stage)  # Add intercept term
Y_second_stage = between_data['lcrmrte']

second_stage = sm.OLS(Y_second_stage, X_second_stage).fit()

print(second_stage.summary())
```

### Julia

```{julia, echo=T, warning=F, message=F}
using StatsModels, Statistics  # For the mean function

# Step 1: Compute the between (average) values for each county
variables = [
    :lcrmrte, :lprbarr, :lpolpc, :lprbpris, :lavgsen,
    :ldensity, :lprbconv, :lpctymle, :lwcon, :lwtuc,
    :lwtrd, :lwfir, :lwser, :lwmfg, :lwfed, :lwsta,
    :lwloc, :west, :central, :urban, :lpctmin, :lmix, :ltaxpc
];

# Step 2: Define endogenous variables, exogenous variables, and instruments
endogenous_vars = [:lprbarr, :lpolpc];

exogenous_vars = [
    :lprbpris, :lavgsen, :ldensity, :lprbconv, :lpctymle,
    :lwcon, :lwtuc, :lwtrd, :lwfir, :lwser, :lwmfg,
    :lwfed, :lwsta, :lwloc, :west, :central, :urban, :lpctmin
];

additional_instruments = [:lmix, :ltaxpc];
instruments = vcat(exogenous_vars, additional_instruments);

# Step 3: First-Stage Regression for 'lprbarr'
formula_lprbarr = @formula(lprbarr ~ lprbpris + lavgsen + ldensity + lprbconv +
                                     lpctymle + lwcon + lwtuc + lwtrd + lwfir +
                                     lwser + lwmfg + lwfed + lwsta + lwloc +
                                     west + central + urban + lpctmin +
                                     lmix + ltaxpc);

model_lprbarr = lm(formula_lprbarr, group_means);
group_means.lprbarr_hat = predict(model_lprbarr);

# Step 4: First-Stage Regression for 'lpolpc'
formula_lpolpc = @formula(lpolpc ~ lprbpris + lavgsen + ldensity + lprbconv +
                                    lpctymle + lwcon + lwtuc + lwtrd + lwfir +
                                    lwser + lwmfg + lwfed + lwsta + lwloc +
                                    west + central + urban + lpctmin +
                                    lmix + ltaxpc);

model_lpolpc = lm(formula_lpolpc, group_means);
group_means.lpolpc_hat = predict(model_lpolpc);

# Step 5: Second-Stage Regression
formula_second_stage = @formula(lcrmrte ~ lprbarr_hat + lpolpc_hat +
                                           lprbpris + lavgsen + ldensity +
                                           lprbconv + lpctymle + lwcon +
                                           lwtuc + lwtrd + lwfir + lwser +
                                           lwmfg + lwfed + lwsta + lwloc +
                                           west + central + urban + lpctmin);

model_second_stage = lm(formula_second_stage, group_means);

coeftable(model_second_stage)
```

:::



## Conclusão



## Conclusão

A aplicação de estimadores de dados em painel, tanto de equações simples quanto simultâneas, permite abordar a heterogeneidade não observada e a simultaneidade convencional. Os resultados indicam que tanto as estratégias de mercado de trabalho quanto as de justiça criminal são importantes para dissuadir o crime. No entanto, a eficácia dos incentivos de aplicação da lei foi amplamente superestimada. Especificamente, os efeitos dissuasores das probabilidades de prisão e condenação são muito menores do que os obtidos a partir de estimativas de corte transversal. Ignorar a heterogeneidade dos condados tende a inflacionar as estimativas dos efeitos dissuasores.

## Críticas ao artigo

Alguma críticas ao artigo em que podemos fazer:

- Não explica o motivo de aplicar log nas variaveis, e nem como aplicou o modelo.
- Não tem análise de resíduos.
- Pouca informação acerca do banco de dados utilizado no artigo.
- Ao se realizar a anáise exploratória, temos nas variáveis que envolvem probabilidade alguns valores muito maiores que 1, ao qual ele não cita e não fala como tratou esses valores.


# Aplicação

Baltagi, B. H., & Pinnoi, N. (1995). Public capital stock and state productivity growth: Further evidence from an error components model. Empirical Economics, 20(2), 351–359.

## Motivação

- A relação entre capital público e desempenho econômico é um tema emergente na economia, com implicações significativas para políticas públicas.
- Este estudo investiga a contribuição de diferentes tipos de infraestrutura pública para a produção privada, utilizando dados de séries temporais e seções cruzadas de 48 estados dos EUA entre 1970 e 1986.
- A pesquisa aborda a variabilidade nas estimativas de contribuição do capital público, destacando a importância de considerar efeitos específicos de cada estado e erros de medição.

## Motivação

- A aplicação de um modelo de produção Cobb-Douglas com efeitos não observáveis permite uma análise mais precisa da relação entre capital público e produtividade.
- A correção de erros de medição é fundamental para obter estimativas consistentes e confiáveis.
- Os resultados podem informar decisões sobre investimentos em infraestrutura, ajudando a maximizar o impacto econômico e a eficiência dos recursos públicos.
- A compreensão dos efeitos de spillover da infraestrutura pública entre estados pode guiar estratégias de desenvolvimento regional.

## Banco de Dados

O banco de dados utilizado contém 816 observações referentes a 48 estados dos EUA, abrangendo o período de 1970 a 1986. As colunas incluídas no conjunto de dados são:

- **state**: Estado
- **year**: Ano
- **region**: Região
- **pcap**: Estoque de capital público
- **hwy**: Infraestrutura de rodovias e ruas
- **water**: Instalações de água e esgoto
- **util**: Outros edifícios e estruturas públicas
- **pc**: Estoque de capital privado
- **gsp**: Produto Interno Bruto Estadual (Gross State Product)
- **emp**: Insumo de trabalho medido pelo emprego em folhas de pagamento não agrícolas
- **unemp**: Taxa de desemprego estadual

Este conjunto de dados permite uma análise detalhada das variáveis econômicas e de infraestrutura ao longo do tempo, possibilitando o controle de fatores regionais e temporais específicos.

EXPLICAR COMO E CALCULADA AS VARIAVEIS E TALVEZ FALAR EM RELACAO AO ERRO DE MEDIDA
Therefore, the estimates of public capital stock are likely to contain measurement error. 

## Modelos

As seguintes especificações log-lineares de Cobb-Douglas são estimadas. Todas as variáveis estão no logaritmo natural, exceto a taxa de desemprego (Unemp).

### Modelo 1

$Y_{it} = \alpha + \beta_1 K_{it} + \beta_2 L_{it} + \beta_3 \text{Unemp}_{it} + u_{it}$

- **$Y_{it}$**: Produção para o estado $i$ no tempo $t$.
- **$K_{it}$**: Insumo de capital para o estado $i$ no tempo $t$.
- **$L_{it}$**: Insumo de trabalho para o estado $i$ no tempo $t$.
- **$\text{Unemp}_{it}$**: Taxa de desemprego para o estado $i$ no tempo $t$ (não em forma logarítmica).
- **$u_{it}$**: Termo de erro capturando fatores não observados que afetam a produção.

### Modelo 2

$Y_{it} = \alpha + \beta_1 K_{it} + \beta_2 L_{it} + \beta_3 KG_{it} + \beta_4 \text{Unemp}_{it} + u_{it}$

- **$KG_{it}$**: Estoque de capital público para o estado $i$ no tempo $t$.

## Modelos

### Modelo 3

$Y_{it} = \alpha + \beta_1 K_{it} + \beta_2 L_{it} + \beta_3 KH_{it} + \beta_4 KW_{it} + \beta_5 KO_{it} + \beta_6 \text{Unemp}_{it} + u_{it}$

- **$KH_{it}$**: Capital relacionado a rodovias e ruas para o estado $i$ no tempo $t$.
- **$KW_{it}$**: Capital relacionado a instalações de água e esgoto para o estado $i$ no tempo $t$.
- **$KO_{it}$**: Capital relacionado a outros edifícios e estruturas públicas para o estado $i$ no tempo $t$.

Neste caso, $u_{it} = \mu_i + \nu_{it}$, onde $\mu_i$ é um efeito específico do estado e $\nu_{it}$ é uma perturbação aleatória clássica. $\mu_i$ pode ser modelado como fixo ou aleatório. Exemplos de tais efeitos específicos de estado incluem a dotação de recursos naturais, a qualidade da infraestrutura pública, características físicas de um estado (por exemplo, estado sem litoral), a capacidade de atrair e utilizar investimento estrangeiro, e os efeitos de rede. Além disso, os efeitos de transbordamento da melhoria da infraestrutura de outros estados podem ser incluídos nos efeitos específicos de estado.
O modelo considera efeitos específicos de estado, que podem ser fixos ou aleatórios, e utiliza Mínimos Quadrados Generalizados Factíveis (FGLS) para lidar com heterocedasticidade e correlação serial. Erros de medição no capital público são abordados usando variáveis instrumentais

## Teoria Macroecômica
  - O quê esperar do aumento dos gastos públicos
  # Teoria Keynesiana
    Para uma economia com setor externo em equilibrio
    
    $Y = C\ + \ I \ + \ G $
    Onde: $Y$ é a renda Nacional
    $C$ é o consumo
    $I$ é o investimeno
    $G$ são os gastos do governo
    A partir disso temos que:
    $C = C_{o} \ + cY_{dt}$
    Onde
    $C_{o}$:É Consumo autônomo
    $c$: a proporção marginal a consumir
    $Y_{dt}$: Renda disponivel
    substituindo no modelo original:
    $Y_{t} = C_{o} \ + cY_{t-1} + \ I_{t} \ + \ G_{t}$
## Teoria Neoclassica
  - a economia já opera em "pleno emprego"
  - Ignora o impacto da politica fiscal
  - Os invesimentos do governo "competem" com os do setor privado(desinsentivo ao investimento privado)
  
## Modelo 1

::: {.panel-tabset}

### R

```{r, echo = T, warning = F, message = F}
library(plm)
data("Produc", package = "plm")
write.csv(Produc, file = "Produc.csv")
pdata <- pdata.frame(Produc, index = c("state", "year"))

print(summary(lm(formula = log(gsp) ~ log(pc) + log(emp) + unemp, data = pdata)))
print(summary(plm(formula = log(gsp) ~ log(pc) + log(emp) + unemp, data = pdata, model = "between")))
print(summary(plm(formula = log(gsp) ~ log(pc) + log(emp) + unemp, data = pdata, model = "within")))
print(summary(plm(log(gsp) ~ log(pc) + log(emp) + unemp, data = pdata, model = "random")))
```


:::


## Modelo 2

::: {.panel-tabset}

### R

```{r}
summary(lm(formula = log(gsp)~ log(pc) + log(emp) + log(pcap) + unemp, data = pdata))
summary(plm(formula = log(gsp)~ log(pc) + log(emp) + log(pcap) + unemp, data = pdata, model = "between"))
summary(plm(formula = log(gsp)~ log(pc) + log(emp) + log(pcap) + unemp, data = pdata, model = "within"))
summary(plm(log(gsp)~ log(pc) + log(emp) + log(pcap) + unemp, data = pdata, model = "random"))
```


:::

## Modelo 3

::: {.panel-tabset}

### R

```{r, echo = T, warning = F, message = F}
summary(lm(formula = log(gsp)~ log(pc) + log(emp) + log(hwy) + log(water) + log(util) + unemp, data = pdata))
summary(plm(formula = log(gsp)~ log(pc) + log(emp) + log(hwy) + log(water) + log(util) + unemp, data = pdata, model = "between"))
summary(plm(formula = log(gsp)~ log(pc) + log(emp) + log(hwy) + log(water) + log(util) + unemp, data = pdata, model = "within"))
summary(plm(log(gsp)~ log(pc) + log(emp) + log(hwy) + log(water) + log(util) + unemp, data = pdata, model = "random"))
```


:::


## Modelo

O teste de Hausman-Taylor (1981) revelou que os erros estão correlacionados com os regressores apenas quando o estoque de capital público é incluído na função de produção. Isso indicou erro de medição no estoque de capital público, tornando as estimativas de OLS, "Within" e FGLS inconsistentes. A aplicação de variáveis instrumentais mostrou que a relação entre capital público e produção privada não é tão robusta quanto sugerido por outros estudos, sendo o efeito positivo, mas insignificante. No entanto, ao decompor o estoque de capital público, componentes como instalações de água e esgoto demonstraram efeitos positivos e significativos na produção privada.


## Modelo envolvendo a primeira diferença

::: {.panel-tabset}

### R

```{r, echo = T, warning = F, message = F}
library(dplyr)
library(tidyverse)
library(plm)

pdata_clean <- na.omit(pdata)

# Calcular a primeira diferença logarítmica para a variável resposta e as covariáveis
pdata_clean$gsp_diff1 <- diff(log(pdata_clean$gsp), lag = 1)
pdata_clean$pc_diff1 <- diff(log(pdata_clean$pc), lag = 1)
pdata_clean$emp_diff1 <- diff(log(pdata_clean$emp), lag = 1)
pdata_clean$pcap_diff1 <- diff(log(pdata_clean$pcap), lag = 1)
pdata_clean$unemp_diff1 <- diff(pdata_clean$unemp, lag = 1)
pdata_clean$hwy_diff1 <- diff(log(pdata_clean$hwy), lag = 1)
pdata_clean$water_diff1 <- diff(log(pdata_clean$water), lag = 1)
pdata_clean$util_diff1 <- diff(log(pdata_clean$util), lag = 1)

modelo2_diff1 <- lm(gsp_diff1 ~ -1 + pc_diff1 + emp_diff1 + pcap_diff1 + unemp_diff1, data = pdata_clean)
summary(modelo2_diff1)

modelo3_diff1 <- lm(gsp_diff1 ~ -1 + pc_diff1 + emp_diff1 + hwy_diff1 + water_diff1 + util_diff1 + unemp_diff1, data = pdata_clean)
summary(modelo3_diff1)
```


:::

## Modelo envolvendo a segunda diferença

::: {.panel-tabset}

### R

```{r, echo = T, warning = F, message = F}
# Calcular a primeira diferença logarítmica para a variável resposta e as covariáveis
pdata_clean$gsp_diff2 <- diff(log(pdata_clean$gsp), lag  = 2)
pdata_clean$pc_diff2 <- diff(log(pdata_clean$pc), lag  = 2)
pdata_clean$emp_diff2 <- diff(log(pdata_clean$emp), lag  = 2)
pdata_clean$pcap_diff2 <- diff(log(pdata_clean$pcap), lag  = 2)
pdata_clean$unemp_diff2 <- diff(pdata_clean$unemp, lag  = 2)
pdata_clean$hwy_diff2 <- diff(log(pdata_clean$hwy), lag = 2)
pdata_clean$water_diff2 <- diff(log(pdata_clean$water), lag = 2)
pdata_clean$util_diff2 <- diff(log(pdata_clean$util), lag = 2)

modelo2_diff2 <- lm(gsp_diff2 ~ -1 + pc_diff2 + emp_diff2 + pcap_diff2 + unemp_diff2, data = pdata_clean)
summary(modelo2_diff2)

modelo3_diff2 <- lm(gsp_diff2 ~ -1 + pc_diff2 + emp_diff2 + hwy_diff2 + water_diff2 + util_diff2 + unemp_diff2, data = pdata_clean)
summary(modelo3_diff2)
```


:::

## Modelo envolvendo a terceira diferença

::: {.panel-tabset}

### R

```{r, echo = T, warning = F, message = F}
# Calcular a  terceira diferença logarítmica para a variável resposta e as covariáveis
pdata_clean$gsp_diff3 <- diff(log(pdata_clean$gsp), lag  = 3)
pdata_clean$pc_diff3 <- diff(log(pdata_clean$pc), lag  = 3)
pdata_clean$emp_diff3 <- diff(log(pdata_clean$emp), lag  = 3)
pdata_clean$pcap_diff3 <- diff(log(pdata_clean$pcap), lag = 3)
pdata_clean$unemp_diff3 <- diff(pdata_clean$unemp, lag  = 3)
pdata_clean$hwy_diff3 <- diff(log(pdata_clean$hwy), lag = 3)
pdata_clean$water_diff3 <- diff(log(pdata_clean$water), lag = 3)
pdata_clean$util_diff3 <- diff(log(pdata_clean$util), lag = 3)

modelo2_diff3 <- lm(gsp_diff3 ~ -1 + pc_diff3 + emp_diff3 + pcap_diff3 + unemp_diff3, data = pdata_clean)
summary(modelo2_diff3)

modelo3_diff3 <- lm(gsp_diff3 ~ -1 + pc_diff3 + emp_diff3 + hwy_diff3 + water_diff3 + util_diff3 + unemp_diff3, data = pdata_clean)
summary(modelo3_diff3)
```


:::

## Modelo envolvendo a quarta diferença

::: {.panel-tabset}

### R

```{r, echo = T, warning = F, message = F}
# Calcular a primeira diferença logarítmica para a variável resposta e as covariáveis
pdata_clean$gsp_diff4 <- diff(log(pdata_clean$gsp), lag  = 4)
pdata_clean$pc_diff4 <- diff(log(pdata_clean$pc), lag  = 4)
pdata_clean$emp_diff4 <- diff(log(pdata_clean$emp), lag  = 4)
pdata_clean$pcap_diff4 <- diff(log(pdata_clean$pcap), lag = 4)
pdata_clean$unemp_diff4 <- diff(pdata_clean$unemp, lag  = 4)
pdata_clean$hwy_diff4 <- diff(log(pdata_clean$hwy), lag = 4)
pdata_clean$water_diff4 <- diff(log(pdata_clean$water), lag = 4)
pdata_clean$util_diff4 <- diff(log(pdata_clean$util), lag = 4)

modelo2_diff4 <- lm(gsp_diff4 ~ -1 + pc_diff4 + emp_diff4 + pcap_diff4 + unemp_diff4, data = pdata_clean)
summary(modelo2_diff4)

modelo3_diff4 <- lm(gsp_diff4 ~ -1 + pc_diff4 + emp_diff4 + hwy_diff4 + water_diff4 + util_diff4 + unemp_diff4, data = pdata_clean)
summary(modelo3_diff4)
```


:::

## Modelo com variáveis instrumentais

::: {.panel-tabset}

### R

```{r}
library(AER)

pdata_clean <- na.omit(pdata)

# Calcular a primeira diferença logarítmica para a variável resposta e as covariáveis
pdata_clean$gsp_diff1 <- diff(log(pdata_clean$gsp), differences = 1)
pdata_clean$pc_diff1 <- diff(log(pdata_clean$pc), differences = 1)
pdata_clean$emp_diff1 <- diff(log(pdata_clean$emp), differences = 1)
pdata_clean$pcap_diff1 <- diff(log(pdata_clean$pcap), differences = 1)
pdata_clean$unemp_diff1 <- diff(pdata_clean$unemp, differences = 1)

# Criar defasagens para usar como instrumentos
pdata_clean$pcap_diff2 <- diff(pdata_clean$pcap, lag = 2)

# Ajustar o modelo de regressão IV usando a primeira diferença
modelo2_iv <- ivreg(gsp_diff1 ~ -1 + pc_diff1 + emp_diff1 + pcap_diff1 + unemp_diff1 |  
                      pc_diff1 + emp_diff1 + unemp_diff1 + pcap_diff2, data = pdata_clean)

summary(modelo2_iv)
```

:::

## Conclusão

- O capital público tem um efeito positivo e significativo na produção privada no longo prazo, mas os efeitos de curto prazo são menos claros.
- A desagregação do capital público revela que rodovias e ruas, bem como instalações de água e esgoto, têm efeitos positivos no curto prazo.
- Os efeitos de outros edifícios públicos são negativos no curto prazo, possivelmente devido a excesso de capacidade em escolas públicas.
- O impacto do capital público total na produção privada é insignificante quando se consideram erros de medição e efeitos específicos de estado.
- No entanto, componentes específicos do capital público, como água e esgoto, mostram contribuições produtivas consistentes.
- O estudo destaca a importância de considerar erros de medição e efeitos específicos de estado em análises econométricas para obter estimativas consistentes.

## Críticas ao artigo

- Nao explicita as diferenças utilizada
- Fala pouco sobre teoria


## Referencias

- Slides Aula Trucios ...
- Cornwell, C., & Trumbull, W. N. (1994). Estimating the Economic Model of Crime with Panel Data. The Review of Economics and Statistics, 76(2), 360–366.
- Baltagi, B. H., & Pinnoi, N. (1995). Public capital stock and state productivity growth: Further evidence from an error components model. Empirical Economics, 20(2), 351–359.



